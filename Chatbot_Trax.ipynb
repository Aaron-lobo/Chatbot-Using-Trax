{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "Chatbot_Trax.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "coursera": {
      "schema_names": [
        "NLPC4-4"
      ]
    },
    "jupytext": {
      "encoding": "# -*- coding: utf-8 -*-",
      "formats": "ipynb,py:percent",
      "text_representation": {
        "extension": ".py",
        "format_name": "percent",
        "format_version": "1.3",
        "jupytext_version": "1.5.2"
      }
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aaron-lobo/Chatbot-Using-Trax/blob/main/Chatbot_Trax.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PytzZn2KzgvR",
        "outputId": "416a3f13-6d9c-48de-9354-0d49e43744fe"
      },
      "source": [
        "!pip install --upgrade tensorflow-probability==0.8\n",
        "!pip install trax\n",
        "!pip install t5==0.8.1\n",
        "!pip list | grep trax"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: tensorflow-probability==0.8 in /usr/local/lib/python3.7/dist-packages (0.8.0)\n",
            "Processing /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd/gast-0.2.2-cp37-none-any.whl\n",
            "Collecting cloudpickle==1.1.1\n",
            "  Using cached https://files.pythonhosted.org/packages/24/fb/4f92f8c0f40a0d728b4f3d5ec5ff84353e705d8ff5e3e447620ea98b06bd/cloudpickle-1.1.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability==0.8) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: decorator in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability==0.8) (4.4.2)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability==0.8) (1.15.0)\n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement gast==0.3.3, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: gym 0.17.3 has requirement cloudpickle<1.7.0,>=1.2.0, but you'll have cloudpickle 1.1.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: gast, cloudpickle\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: cloudpickle 1.6.0\n",
            "    Uninstalling cloudpickle-1.6.0:\n",
            "      Successfully uninstalled cloudpickle-1.6.0\n",
            "Successfully installed cloudpickle-1.1.1 gast-0.2.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gast"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: trax in /usr/local/lib/python3.7/dist-packages (1.3.8)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from trax) (1.15.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from trax) (3.2.2)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.7/dist-packages (from trax) (0.2.13)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from trax) (1.4.1)\n",
            "Requirement already satisfied: t5 in /usr/local/lib/python3.7/dist-packages (from trax) (0.8.1)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.7/dist-packages (from trax) (0.1.66+cuda110)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (from trax) (0.17.3)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from trax) (0.12.0)\n",
            "Requirement already satisfied: gin-config in /usr/local/lib/python3.7/dist-packages (from trax) (0.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from trax) (1.19.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from trax) (5.4.8)\n",
            "Requirement already satisfied: tensorflow-text in /usr/local/lib/python3.7/dist-packages (from trax) (2.4.3)\n",
            "Requirement already satisfied: funcsigs in /usr/local/lib/python3.7/dist-packages (from trax) (1.0.2)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.7/dist-packages (from trax) (4.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->trax) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->trax) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->trax) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->trax) (0.10.0)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.7/dist-packages (from jax->trax) (3.3.0)\n",
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.7/dist-packages (from t5->trax) (1.5.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from t5->trax) (3.2.5)\n",
            "Requirement already satisfied: rouge-score in /usr/local/lib/python3.7/dist-packages (from t5->trax) (0.0.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from t5->trax) (1.8.1+cu101)\n",
            "Requirement already satisfied: transformers>=2.7.0 in /usr/local/lib/python3.7/dist-packages (from t5->trax) (4.6.1)\n",
            "Requirement already satisfied: mesh-tensorflow[transformer]>=0.1.13 in /usr/local/lib/python3.7/dist-packages (from t5->trax) (0.1.19)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from t5->trax) (0.1.95)\n",
            "Requirement already satisfied: tfds-nightly in /usr/local/lib/python3.7/dist-packages (from t5->trax) (4.3.0.dev202105200107)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from t5->trax) (1.1.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from t5->trax) (0.22.2.post1)\n",
            "Requirement already satisfied: babel in /usr/local/lib/python3.7/dist-packages (from t5->trax) (2.9.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.7/dist-packages (from jaxlib->trax) (1.12)\n",
            "Collecting cloudpickle<1.7.0,>=1.2.0\n",
            "  Using cached https://files.pythonhosted.org/packages/e7/e3/898487e5dbeb612054cf2e0c188463acb358167fef749c53c8bb8918cea1/cloudpickle-1.6.0-py3-none-any.whl\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym->trax) (1.5.0)\n",
            "Requirement already satisfied: tensorflow<2.5,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text->trax) (2.4.1)\n",
            "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text->trax) (0.12.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (0.1.6)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (0.30.0)\n",
            "Requirement already satisfied: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (5.1.2)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (21.2.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (0.3.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (4.41.1)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (2.3)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (3.12.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (0.16.0)\n",
            "Requirement already satisfied: portalocker==2.0.0 in /usr/local/lib/python3.7/dist-packages (from sacrebleu->t5->trax) (2.0.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->t5->trax) (3.7.4.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=2.7.0->t5->trax) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers>=2.7.0->t5->trax) (4.0.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers>=2.7.0->t5->trax) (0.0.45)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers>=2.7.0->t5->trax) (20.9)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers>=2.7.0->t5->trax) (0.10.2)\n",
            "Requirement already satisfied: huggingface-hub==0.0.8 in /usr/local/lib/python3.7/dist-packages (from transformers>=2.7.0->t5->trax) (0.0.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>=2.7.0->t5->trax) (3.0.12)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->t5->trax) (2018.9)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->t5->trax) (1.0.1)\n",
            "Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (2.10.0)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (2.4.1)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (1.32.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (2.4.0)\n",
            "Collecting gast==0.3.3\n",
            "  Using cached https://files.pythonhosted.org/packages/d6/84/759f5dd23fec8ba71952d97bcc7e2c9d7d63bdc582421f3cd4be845f0c98/gast-0.3.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (0.2.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (0.36.2)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (1.6.3)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (1.1.2)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (1.12.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-metadata->tensorflow-datasets->trax) (1.53.0)\n",
            "Requirement already satisfied: zipp>=0.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-resources; python_version < \"3.9\"->tensorflow-datasets->trax) (3.4.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets->trax) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets->trax) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets->trax) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets->trax) (1.24.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.6.1->tensorflow-datasets->trax) (56.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=2.7.0->t5->trax) (8.0.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (0.4.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (1.8.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (1.30.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (3.3.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (2.0.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (1.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (4.2.2)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->trax) (0.4.8)\n",
            "\u001b[31mERROR: tensorflow-probability 0.8.0 has requirement cloudpickle==1.1.1, but you'll have cloudpickle 1.6.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-probability 0.8.0 has requirement gast<0.3,>=0.2, but you'll have gast 0.3.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: cloudpickle, gast\n",
            "  Found existing installation: cloudpickle 1.1.1\n",
            "    Uninstalling cloudpickle-1.1.1:\n",
            "      Successfully uninstalled cloudpickle-1.1.1\n",
            "  Found existing installation: gast 0.2.2\n",
            "    Uninstalling gast-0.2.2:\n",
            "      Successfully uninstalled gast-0.2.2\n",
            "Successfully installed cloudpickle-1.6.0 gast-0.3.3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gast"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: t5==0.8.1 in /usr/local/lib/python3.7/dist-packages (0.8.1)\n",
            "Requirement already satisfied: mesh-tensorflow[transformer]>=0.1.13 in /usr/local/lib/python3.7/dist-packages (from t5==0.8.1) (0.1.19)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from t5==0.8.1) (1.8.1+cu101)\n",
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.7/dist-packages (from t5==0.8.1) (1.5.1)\n",
            "Requirement already satisfied: rouge-score in /usr/local/lib/python3.7/dist-packages (from t5==0.8.1) (0.0.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from t5==0.8.1) (1.1.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from t5==0.8.1) (0.22.2.post1)\n",
            "Requirement already satisfied: tfds-nightly in /usr/local/lib/python3.7/dist-packages (from t5==0.8.1) (4.3.0.dev202105200107)\n",
            "Requirement already satisfied: gin-config in /usr/local/lib/python3.7/dist-packages (from t5==0.8.1) (0.4.0)\n",
            "Requirement already satisfied: six>=1.14 in /usr/local/lib/python3.7/dist-packages (from t5==0.8.1) (1.15.0)\n",
            "Requirement already satisfied: tensorflow-text in /usr/local/lib/python3.7/dist-packages (from t5==0.8.1) (2.4.3)\n",
            "Requirement already satisfied: transformers>=2.7.0 in /usr/local/lib/python3.7/dist-packages (from t5==0.8.1) (4.6.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from t5==0.8.1) (3.2.5)\n",
            "Requirement already satisfied: babel in /usr/local/lib/python3.7/dist-packages (from t5==0.8.1) (2.9.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from t5==0.8.1) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from t5==0.8.1) (1.19.5)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from t5==0.8.1) (0.1.95)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from t5==0.8.1) (0.12.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from mesh-tensorflow[transformer]>=0.1.13->t5==0.8.1) (0.16.0)\n",
            "Requirement already satisfied: tensorflow-datasets; extra == \"transformer\" in /usr/local/lib/python3.7/dist-packages (from mesh-tensorflow[transformer]>=0.1.13->t5==0.8.1) (4.0.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->t5==0.8.1) (3.7.4.3)\n",
            "Requirement already satisfied: portalocker==2.0.0 in /usr/local/lib/python3.7/dist-packages (from sacrebleu->t5==0.8.1) (2.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->t5==0.8.1) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->t5==0.8.1) (2018.9)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->t5==0.8.1) (1.0.1)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tfds-nightly->t5==0.8.1) (2.3)\n",
            "Requirement already satisfied: protobuf>=3.12.2 in /usr/local/lib/python3.7/dist-packages (from tfds-nightly->t5==0.8.1) (3.12.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from tfds-nightly->t5==0.8.1) (4.41.1)\n",
            "Requirement already satisfied: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.7/dist-packages (from tfds-nightly->t5==0.8.1) (5.1.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tfds-nightly->t5==0.8.1) (0.3.3)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.7/dist-packages (from tfds-nightly->t5==0.8.1) (21.2.0)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tfds-nightly->t5==0.8.1) (0.30.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from tfds-nightly->t5==0.8.1) (2.23.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from tfds-nightly->t5==0.8.1) (1.1.0)\n",
            "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text->t5==0.8.1) (0.12.0)\n",
            "Requirement already satisfied: tensorflow<2.5,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text->t5==0.8.1) (2.4.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers>=2.7.0->t5==0.8.1) (4.0.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers>=2.7.0->t5==0.8.1) (20.9)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers>=2.7.0->t5==0.8.1) (0.10.2)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers>=2.7.0->t5==0.8.1) (0.0.45)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>=2.7.0->t5==0.8.1) (3.0.12)\n",
            "Requirement already satisfied: huggingface-hub==0.0.8 in /usr/local/lib/python3.7/dist-packages (from transformers>=2.7.0->t5==0.8.1) (0.0.8)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=2.7.0->t5==0.8.1) (2019.12.20)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets; extra == \"transformer\"->mesh-tensorflow[transformer]>=0.1.13->t5==0.8.1) (0.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.12.2->tfds-nightly->t5==0.8.1) (56.1.0)\n",
            "Requirement already satisfied: zipp>=0.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-resources; python_version < \"3.9\"->tfds-nightly->t5==0.8.1) (3.4.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-metadata->tfds-nightly->t5==0.8.1) (1.53.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tfds-nightly->t5==0.8.1) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tfds-nightly->t5==0.8.1) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tfds-nightly->t5==0.8.1) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tfds-nightly->t5==0.8.1) (2020.12.5)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->t5==0.8.1) (3.3.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->t5==0.8.1) (1.12.1)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->t5==0.8.1) (0.3.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->t5==0.8.1) (2.4.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->t5==0.8.1) (0.36.2)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->t5==0.8.1) (1.32.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->t5==0.8.1) (1.6.3)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->t5==0.8.1) (2.4.1)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->t5==0.8.1) (1.12)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->t5==0.8.1) (0.2.0)\n",
            "Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->t5==0.8.1) (2.10.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->t5==0.8.1) (1.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers>=2.7.0->t5==0.8.1) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=2.7.0->t5==0.8.1) (8.0.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->t5==0.8.1) (2.0.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->t5==0.8.1) (0.4.4)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->t5==0.8.1) (1.30.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->t5==0.8.1) (3.3.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->t5==0.8.1) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->t5==0.8.1) (1.3.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->t5==0.8.1) (4.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->t5==0.8.1) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->t5==0.8.1) (4.7.2)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->t5==0.8.1) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->t5==0.8.1) (0.4.8)\n",
            "trax                          1.3.8                \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aV4zpTnSVFIp",
        "outputId": "0f321755-7f3e-4862-84ce-45ba31a09e60"
      },
      "source": [
        "import json\n",
        "import random\n",
        "import numpy as np\n",
        "from termcolor import colored\n",
        " \n",
        "import trax\n",
        "from trax import layers as tl\n",
        "from trax.supervised import training\n",
        "\n",
        "!pip list | grep trax\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "trax                          1.3.8                \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvvwnSYc0Xxy",
        "outputId": "7b727562-125f-46fa-c0c4-4e545d7d146b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YkPSpXSzgvU"
      },
      "source": [
        "DATA_FILE = 'data.json'\n",
        "DATA_DIR = '/content/drive/My Drive/ChatbotWork/data'\n",
        "\n",
        "DIALOGUE_DB = {}\n",
        "\n",
        "VOCAB_FILE = 'en_32k.subword'\n",
        "VOCAB_DIR = '/content/drive/My Drive/ChatbotWork/data/vocabs'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K58I5vFB7GlP"
      },
      "source": [
        "def load_json(directory, file):\n",
        "    with open(f'{directory}/{file}') as file: \n",
        "        db = json.load(file)\n",
        "    return db\n",
        "\n",
        "DIALOGUE_DB = load_json(DATA_DIR, DATA_FILE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGBnUfEk8p9x",
        "outputId": "08dc0e49-8c35-4869-9475-8a2efbe183c8"
      },
      "source": [
        "print(f'The number of dialogues is: {len(DIALOGUE_DB)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of dialogues is: 10438\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MTqW-ekzgvZ"
      },
      "source": [
        "def get_conversation(file, data_db):\n",
        "    result = ''\n",
        "    \n",
        "    len_msg_log = len(data_db[file]['log'])\n",
        "    \n",
        "    delimiter_1 = ' Person 1: '\n",
        "    delimiter_2 = ' Person 2: '\n",
        "    \n",
        "    for i in range(len_msg_log):\n",
        "        cur_log = data_db[file]['log'][i]\n",
        "        \n",
        "        if i%2 == 0:                   \n",
        "            result += delimiter_1\n",
        "        else: \n",
        "            result += delimiter_2\n",
        "        \n",
        "        result += cur_log['text']\n",
        "    \n",
        "    return result\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ugvx0noP8p-G",
        "outputId": "6b89e3b2-c63d-4c00-e8e8-3c1d22679004"
      },
      "source": [
        "file = 'SNG01856.json'\n",
        "conversation = get_conversation(file, DIALOGUE_DB)\n",
        "\n",
        "print(conversation)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Person 1: am looking for a place to to stay that has cheap price range it should be in a type of hotel Person 2: Okay, do you have a specific area you want to stay in? Person 1: no, i just need to make sure it's cheap. oh, and i need parking Person 2: I found 1 cheap hotel for you that includes parking. Do you like me to book it? Person 1: Yes, please. 6 people 3 nights starting on tuesday. Person 2: I am sorry but I wasn't able to book that for you for Tuesday. Is there another day you would like to stay or perhaps a shorter stay? Person 1: how about only 2 nights. Person 2: Booking was successful.\n",
            "Reference number is : 7GAWK763. Anything else I can do for you? Person 1: No, that will be all. Good bye. Person 2: Thank you for using our services.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8SGrkexzgvb",
        "outputId": "84c59cc7-1d0f-46f6-9e2b-2f577675b1a2"
      },
      "source": [
        "def print_conversation(conversation):\n",
        "    \n",
        "    delimiter_1 = 'Person 1: '\n",
        "    delimiter_2 = 'Person 2: '\n",
        "    \n",
        "    split_list_d1 = conversation.split(delimiter_1)\n",
        "    \n",
        "    for sublist in split_list_d1[1:]:\n",
        "        split_list_d2 = sublist.split(delimiter_2)\n",
        "        print(colored(f'Person 1: {split_list_d2[0]}', 'red'))\n",
        "        \n",
        "        if len(split_list_d2) > 1:\n",
        "            print(colored(f'Person 2: {split_list_d2[1]}', 'green'))\n",
        "\n",
        "            \n",
        "print_conversation(conversation)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[31mPerson 1: am looking for a place to to stay that has cheap price range it should be in a type of hotel \u001b[0m\n",
            "\u001b[32mPerson 2: Okay, do you have a specific area you want to stay in? \u001b[0m\n",
            "\u001b[31mPerson 1: no, i just need to make sure it's cheap. oh, and i need parking \u001b[0m\n",
            "\u001b[32mPerson 2: I found 1 cheap hotel for you that includes parking. Do you like me to book it? \u001b[0m\n",
            "\u001b[31mPerson 1: Yes, please. 6 people 3 nights starting on tuesday. \u001b[0m\n",
            "\u001b[32mPerson 2: I am sorry but I wasn't able to book that for you for Tuesday. Is there another day you would like to stay or perhaps a shorter stay? \u001b[0m\n",
            "\u001b[31mPerson 1: how about only 2 nights. \u001b[0m\n",
            "\u001b[32mPerson 2: Booking was successful.\n",
            "Reference number is : 7GAWK763. Anything else I can do for you? \u001b[0m\n",
            "\u001b[31mPerson 1: No, that will be all. Good bye. \u001b[0m\n",
            "\u001b[32mPerson 2: Thank you for using our services.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IrnQ9eNV8p-k",
        "outputId": "f85ed00b-2897-4405-ed0d-981fea41891f"
      },
      "source": [
        "all_files = DIALOGUE_DB.keys()\n",
        "\n",
        "untokenized_data = []\n",
        "\n",
        "for file in all_files:\n",
        "    result = get_conversation(file, DIALOGUE_DB)\n",
        "    \n",
        "    untokenized_data.append(result)\n",
        "\n",
        "print(untokenized_data[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Person 1: am looking for a place to to stay that has cheap price range it should be in a type of hotel Person 2: Okay, do you have a specific area you want to stay in? Person 1: no, i just need to make sure it's cheap. oh, and i need parking Person 2: I found 1 cheap hotel for you that includes parking. Do you like me to book it? Person 1: Yes, please. 6 people 3 nights starting on tuesday. Person 2: I am sorry but I wasn't able to book that for you for Tuesday. Is there another day you would like to stay or perhaps a shorter stay? Person 1: how about only 2 nights. Person 2: Booking was successful.\n",
            "Reference number is : 7GAWK763. Anything else I can do for you? Person 1: No, that will be all. Good bye. Person 2: Thank you for using our services.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKibDdUzzgvh"
      },
      "source": [
        "Split to train and eval data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "buE0b8bjx_p_",
        "outputId": "84fb1658-5d63-40aa-b78a-e97aafd694ae"
      },
      "source": [
        "random.shuffle(untokenized_data)\n",
        "\n",
        "# cutoff (5% of the total length\n",
        "cut_off = int(len(untokenized_data) * .05)\n",
        "\n",
        "train_data, eval_data = untokenized_data[:-cut_off], untokenized_data[-cut_off:]\n",
        "\n",
        "print(f'number of conversations in the data set: {len(untokenized_data)}')\n",
        "print(f'number of conversations in train set: {len(train_data)}')\n",
        "print(f'number of conversations in eval set: {len(eval_data)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of conversations in the data set: 10438\n",
            "number of conversations in train set: 9917\n",
            "number of conversations in eval set: 521\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RuWlmoR9zgvh"
      },
      "source": [
        "def stream(data):\n",
        "    while True:\n",
        "        d = random.choice(data)\n",
        "        \n",
        "        yield (d, d)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZgK5FAAWwOu"
      },
      "source": [
        "data_pipeline = trax.data.Serial(\n",
        "    trax.data.Shuffle(),\n",
        "    \n",
        "    trax.data.Tokenize(vocab_dir=VOCAB_DIR,\n",
        "                       vocab_file=VOCAB_FILE),\n",
        "    \n",
        "    trax.data.FilterByLength(2048),\n",
        "    \n",
        "    trax.data.BucketByLength(boundaries=[128, 256,  512, 1024],\n",
        "                             batch_sizes=[16,    8,    4,   2, 1]),\n",
        "    \n",
        "    trax.data.AddLossWeights(id_to_mask=0)\n",
        ")\n",
        "\n",
        "train_stream = data_pipeline(stream(train_data))\n",
        "eval_stream = data_pipeline(stream(eval_data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9iBQEvhLYRot",
        "outputId": "55862b90-8fec-4114-e92b-3270aedfaa6f"
      },
      "source": [
        "inp, _, _ = next(train_stream)\n",
        "\n",
        "print(\"input shape: \", inp.shape)\n",
        "\n",
        "print(trax.data.detokenize(inp[0], vocab_dir=VOCAB_DIR, vocab_file=VOCAB_FILE))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input shape:  (4, 512)\n",
            " Person 1: Hi, can you fill me in on some details about a place called jesus green outdoor pool, please? Person 2: It's a swimming pool in the North. Phone # is 01223302579 and it's between victoria road and the river. Anything else I can help you with? Person 1: That is the full address that you mentioned? Person 2: Sure.  The address showing is between victoria road and the river.  Again, their phone number is 0122330257.  Can I help with anything else? Person 1: Yes I'd like some info on a place to eat in the centre. Person 2: There are many to choose from! Did you have a type of food or price range in mind? I like Shanghai Family Restaurant, myself. It's a moderately-priced Chinese restaurant. Person 1: I'd like an expensive world food restaurant. Person 2: I am not showing an expensive world restaurant in the centre.  Would you like to try other areas? Person 1: How about one that serves gastropub food instead? Person 2: I have two places in the Centre fitting that criteria.  Person 1: Is it expensive? Person 2: Yes, both of the gastropubs in the center of town are expensive. I recommend the BackStreet Bistro.  Person 1: That sounds good.  Can you book it for 4 at 12:15 on Friday, please? Person 2: I was able to book it reference number is T2VZ7MJU Person 1: Great!  I also need a taxi to commute between the swimming pool and the restaurant, please. Person 2: You are welcome! Please call us back if you need anything further.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adX2eU762BkF"
      },
      "source": [
        "def reversible_layer_forward(x, f, g):\n",
        "    x1, x2 = np.split(x, 2, axis=-1) \n",
        "        \n",
        "    y1 = x1 + f(x2)\n",
        "    y2 = x2 + g(y1)\n",
        "    \n",
        "    y = np.concatenate([y1, y2], axis=-1)\n",
        "    \n",
        "    return y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xTCG9WlaiiO"
      },
      "source": [
        "def reversible_layer_reverse(y, f, g):\n",
        "    \n",
        "    y1, y2 = np.split(y, 2, axis=-1)\n",
        "        \n",
        "    x2 = y2 - g(y1)\n",
        "    x1 = y1 - f(x2)\n",
        "    \n",
        "    x = np.concatenate([x1, x2], axis=-1)\n",
        "    \n",
        "    return x\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjElqUtc2BYG"
      },
      "source": [
        "f = lambda x: x + np.random.uniform(size=x.shape)\n",
        "g = lambda x: x * 3\n",
        "\n",
        "input_vector = np.random.uniform(size=(32,))\n",
        "output_vector = reversible_layer_forward(input_vector, f, g)\n",
        "reversed_vector = reversible_layer_reverse(output_vector, f, g)\n",
        "\n",
        "assert not np.allclose(reversed_vector, input_vector) \n",
        "\n",
        "\n",
        "random_seed = 27686\n",
        "rng = trax.fastmath.random.get_prng(random_seed)\n",
        "f = lambda x: x + trax.fastmath.random.uniform(key=rng, shape=x.shape)\n",
        "\n",
        "output_vector = reversible_layer_forward(input_vector, f, g)\n",
        "reversed_vector = reversible_layer_reverse(output_vector, f, g)\n",
        "\n",
        "assert np.allclose(reversed_vector, input_vector,  atol=1e-07) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RidbAcoR6duP"
      },
      "source": [
        "def ReformerLM(vocab_size=33000, n_layers=2, mode='train', attention_type=tl.SelfAttention):\n",
        "    \n",
        "    model = trax.models.reformer.ReformerLM( \n",
        "        vocab_size=vocab_size,\n",
        "        n_layers=n_layers,\n",
        "        mode=mode,\n",
        "        attention_type=attention_type\n",
        "    )\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hTzGM0pzgvl",
        "outputId": "81eacdd7-947f-433b-ce5f-ffa17248eb40"
      },
      "source": [
        "temp_model = ReformerLM('train')\n",
        "print(str(temp_model))\n",
        "\n",
        "del temp_model "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Serial[\n",
            "  Serial[\n",
            "    ShiftRight(1)\n",
            "  ]\n",
            "  Embedding_train_512\n",
            "  Dropout\n",
            "  Serial[\n",
            "    PositionalEncoding\n",
            "  ]\n",
            "  Dup_out2\n",
            "  ReversibleSerial_in2_out2[\n",
            "    ReversibleHalfResidualDecoderAttn_in2_out2[\n",
            "      Serial[\n",
            "        LayerNorm\n",
            "      ]\n",
            "      SelfAttention\n",
            "    ]\n",
            "    ReversibleSwap_in2_out2\n",
            "    ReversibleHalfResidualDecoderFF_in2_out2[\n",
            "      Serial[\n",
            "        LayerNorm\n",
            "        Dense_2048\n",
            "        Dropout\n",
            "        Serial[\n",
            "          FastGelu\n",
            "        ]\n",
            "        Dense_512\n",
            "        Dropout\n",
            "      ]\n",
            "    ]\n",
            "    ReversibleSwap_in2_out2\n",
            "    ReversibleHalfResidualDecoderAttn_in2_out2[\n",
            "      Serial[\n",
            "        LayerNorm\n",
            "      ]\n",
            "      SelfAttention\n",
            "    ]\n",
            "    ReversibleSwap_in2_out2\n",
            "    ReversibleHalfResidualDecoderFF_in2_out2[\n",
            "      Serial[\n",
            "        LayerNorm\n",
            "        Dense_2048\n",
            "        Dropout\n",
            "        Serial[\n",
            "          FastGelu\n",
            "        ]\n",
            "        Dense_512\n",
            "        Dropout\n",
            "      ]\n",
            "    ]\n",
            "    ReversibleSwap_in2_out2\n",
            "  ]\n",
            "  Concatenate_in2\n",
            "  LayerNorm\n",
            "  Dropout\n",
            "  Serial[\n",
            "    Dense_train\n",
            "  ]\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQehGhoD4Psl"
      },
      "source": [
        "def training_loop(ReformerLM, train_gen, eval_gen, output_dir = \"./model/\"):\n",
        "\n",
        "    lr_schedule = trax.lr.warmup_and_rsqrt_decay(\n",
        "        n_warmup_steps=1000, max_value=0.01)\n",
        "    \n",
        "    \n",
        "    train_task = training.TrainTask(            \n",
        "        \n",
        "        labeled_data=train_gen,\n",
        "        \n",
        "        loss_layer=tl.CrossEntropyLoss(),\n",
        "        \n",
        "        optimizer=trax.optimizers.Adam(0.01),\n",
        "        \n",
        "        lr_schedule=lr_schedule,\n",
        "        \n",
        "        n_steps_per_checkpoint=10\n",
        "    )\n",
        "\n",
        "    \n",
        "    eval_task = training.EvalTask(                      \n",
        "        \n",
        "        labeled_data=eval_gen,\n",
        "        \n",
        "        metrics=[tl.CrossEntropyLoss(), tl.Accuracy()]\n",
        "    )\n",
        "\n",
        "    loop = training.Loop(ReformerLM(mode='train'),\n",
        "                         train_task,\n",
        "                         eval_tasks=[eval_task],\n",
        "                         output_dir=output_dir)\n",
        "\n",
        "    return loop"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrP7jy2ezgvo"
      },
      "source": [
        "def attention(*args, **kwargs):\n",
        "    kwargs['predict_mem_len'] = 120\n",
        "    kwargs['predict_drop_len'] = 120\n",
        "    return tl.SelfAttention(*args, **kwargs)\n",
        "\n",
        "model = ReformerLM(\n",
        "    vocab_size=33000,\n",
        "    n_layers=6,\n",
        "    mode='predict',\n",
        "    attention_type=attention,\n",
        ")\n",
        "\n",
        "shape11 = trax.shapes.ShapeDtype((1, 1), dtype=np.int32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5fEqu1ozgvo"
      },
      "source": [
        "model.init_from_file('/content/drive/My Drive/ChatbotWork/chatbot_model1.pkl.gz', weights_only=True, input_signature=shape11)\n",
        "\n",
        "STARTING_STATE = model.state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-cD3OICzgvo"
      },
      "source": [
        "def tokenize(sentence, vocab_file, vocab_dir):\n",
        "    return list(trax.data.tokenize(iter([sentence]), vocab_file=vocab_file, vocab_dir=vocab_dir))[0]\n",
        "\n",
        "def detokenize(tokens, vocab_file, vocab_dir):\n",
        "    return trax.data.detokenize(tokens, vocab_file=vocab_file, vocab_dir=vocab_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFkjSAOpzgvp"
      },
      "source": [
        "def ReformerLM_output_gen(ReformerLM, start_sentence, vocab_file, vocab_dir, temperature):\n",
        "    \n",
        "    input_tokens = tokenize(start_sentence, vocab_file=vocab_file, vocab_dir=vocab_dir)\n",
        "    \n",
        "    input_tokens_with_batch = np.array(input_tokens)[None, :]\n",
        "    \n",
        "    output_gen = trax.supervised.decoding.autoregressive_sample_stream( \n",
        "        ReformerLM,\n",
        "        inputs=input_tokens_with_batch,\n",
        "        temperature=temperature\n",
        "    )\n",
        "        \n",
        "    return output_gen"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQ_JHCMbzgvp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ce10c1c2-2e2a-4d1a-b8cb-909e6acd344b"
      },
      "source": [
        "import pickle\n",
        "\n",
        "WEIGHTS_FROM_FILE = ()\n",
        "\n",
        "with open('/content/drive/My Drive/ChatbotWork/weights', 'rb') as file:\n",
        "    WEIGHTS_FROM_FILE = pickle.load(file)\n",
        "\n",
        "shape11 = trax.shapes.ShapeDtype((1, 1), dtype=np.int32)\n",
        "\n",
        "def attention(*args, **kwargs):\n",
        "    kwargs['predict_mem_len'] = 120\n",
        "    kwargs['predict_drop_len'] = 120\n",
        "    return tl.SelfAttention(*args, **kwargs)\n",
        "\n",
        "test_model = ReformerLM(vocab_size=5, n_layers=1, mode='predict', attention_type=attention)\n",
        "\n",
        "test_output_gen = ReformerLM_output_gen(test_model, \"test\", vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, temperature=0)\n",
        "\n",
        "test_model.init_weights_and_state(shape11)\n",
        "\n",
        "test_model.weights = WEIGHTS_FROM_FILE\n",
        "\n",
        "output = []\n",
        "\n",
        "for i in range(6):\n",
        "    output.append(next(test_output_gen)[0])\n",
        "\n",
        "print(output)\n",
        "\n",
        "del test_model \n",
        "del WEIGHTS_FROM_FILE\n",
        "del test_output_gen"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-9c2fdaa0fdf0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_weights_and_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mtest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWEIGHTS_FROM_FILE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/trax/layers/base.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, attr, value)\u001b[0m\n\u001b[1;32m    705\u001b[0m           f'attribues, not {attr}.')\n\u001b[1;32m    706\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m       \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/trax/layers/base.py\u001b[0m in \u001b[0;36mweights\u001b[0;34m(self, weights)\u001b[0m\n\u001b[1;32m    452\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mn_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m         raise ValueError(\n\u001b[0;32m--> 454\u001b[0;31m             \u001b[0;34mf'Number of weight elements ({len(weights)}) does not equal the '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m             f'number of sublayers ({n_layers}) in: {str(self)}.')\n\u001b[1;32m    456\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0msublayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msublayer_weights\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msublayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Number of weight elements (11) does not equal the number of sublayers (10) in: Serial[\n  Serial[\n    ShiftRight(1)\n  ]\n  Embedding_5_512\n  Dropout\n  Serial[\n    PositionalEncoding\n  ]\n  Dup_out2\n  ReversibleSerial_in2_out2[\n    ReversibleHalfResidualDecoderAttn_in2_out2[\n      Serial[\n        LayerNorm\n      ]\n      SelfAttention\n    ]\n    ReversibleSwap_in2_out2\n    ReversibleHalfResidualDecoderFF_in2_out2[\n      Serial[\n        LayerNorm\n        Dense_2048\n        Dropout\n        Serial[\n          FastGelu\n        ]\n        Dense_512\n        Dropout\n      ]\n    ]\n    ReversibleSwap_in2_out2\n  ]\n  Concatenate_in2\n  LayerNorm\n  Dropout\n  Serial[\n    Dense_5\n  ]\n]."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQK4inVIzgvq"
      },
      "source": [
        "shape11 = trax.shapes.ShapeDtype((1, 1), dtype=np.int32)\n",
        "\n",
        "def attention(*args, **kwargs):\n",
        "    kwargs['predict_mem_len'] = 120 \n",
        "    kwargs['predict_drop_len'] = 120 \n",
        "    return tl.SelfAttention(*args, **kwargs)\n",
        "\n",
        "model = ReformerLM(\n",
        "    vocab_size=33000,\n",
        "    n_layers=6,\n",
        "    mode='predict',\n",
        "    attention_type=attention,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqjGL-48zgvq"
      },
      "source": [
        "model.init_from_file('/content/drive/My Drive/ChatbotWork/chatbot_model1.pkl.gz',\n",
        "                     weights_only=True, input_signature=shape11)\n",
        "\n",
        "STARTING_STATE = model.state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmWUkDSazgvq"
      },
      "source": [
        "def generate_dialogue(ReformerLM, model_state, start_sentence, vocab_file, vocab_dir, max_len, temperature):\n",
        "    \n",
        "    delimiter_1 = 'Person 1: ' \n",
        "    delimiter_2 = 'Person 2: '\n",
        "    \n",
        "    sentence = ''\n",
        "    \n",
        "    counter = 0\n",
        "    \n",
        "    result = [tokenize(': ', vocab_file=vocab_file, vocab_dir=vocab_dir)]\n",
        "    \n",
        "    ReformerLM.state = model_state\n",
        "    \n",
        "    output = ReformerLM_output_gen(ReformerLM, start_sentence, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, temperature=temperature)\n",
        "    \n",
        "    print(start_sentence.split(delimiter_2)[0].strip())\n",
        "    \n",
        "    for o in output:\n",
        "        \n",
        "        result.append(o)\n",
        "        \n",
        "        sentence = detokenize(np.concatenate(result, axis=0), vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)\n",
        "        \n",
        "        if sentence.endswith(delimiter_1):\n",
        "            sentence = sentence.split(delimiter_1)[0]\n",
        "            print(f'{delimiter_2}{sentence}')\n",
        "            sentence = ''\n",
        "            result.clear()\n",
        "        \n",
        "        elif sentence.endswith(delimiter_2):\n",
        "            sentence = sentence.split(delimiter_2)[0]\n",
        "            print(f'{delimiter_1}{sentence}')\n",
        "            sentence = ''\n",
        "            result.clear()\n",
        "\n",
        "        counter += 1\n",
        "        \n",
        "        if counter > max_len:\n",
        "            break    \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pIZP9VCkzgvr",
        "outputId": "2618fc5a-433a-4501-f8d8-25436a225d14"
      },
      "source": [
        "sample_sentence = ' Person 1: Are there theatres in town? Person 2: '\n",
        "generate_dialogue(ReformerLM=model, model_state=STARTING_STATE, start_sentence=sample_sentence, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, max_len=120, temperature=0.2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Person 1: Are there theatres in town?\n",
            "Person 2: : There are 4 theatres in town. Do you have a preference on area? \n",
            "Person 1: No, but I would like the one in the south. \n",
            "Person 2: I would recommend the Junction. Would you like more information on it? \n",
            "Person 1: Yes, I would like the phone number and postcode please. \n",
            "Person 1: The post code is cb17gx. The phone number is 01223511511. Can I help you with anything else? Person 43, Fensounds good! \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "maLen3SCzgvr",
        "outputId": "8b2690c1-2873-4002-d44b-f16b6ba9eb72"
      },
      "source": [
        "sample_sentence = ' Person 1: Is there a college nearby? Person 2: '\n",
        "generate_dialogue(ReformerLM=model, model_state=STARTING_STATE, start_sentence=sample_sentence, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, max_len=120, temperature=0.2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Person 1: Is there a college nearby?\n",
            "Person 2: : There are 18 colleges in Cambridge. Is there a certain area you are looking for? \n",
            "Person 1: No, I don't care. Can you recommend one and give me the address, postcode, and entrance fee for one of them? \n",
            "Person 2: I'm sorry, but I don't have any information on the entrance fees. There is no entrance fee. \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSfjf26rzgvr",
        "outputId": "35669bbf-6e00-4274-a226-54a7a6e10c3e"
      },
      "source": [
        "sample_sentence = ' Person 1: Can you book a taxi? Person 2: '\n",
        "generate_dialogue(ReformerLM=model, model_state=STARTING_STATE, start_sentence=sample_sentence, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, max_len=120, temperature=0.2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Person 1: Can you book a taxi?\n",
            "Person 2: : I sure can. Where are you going? \n",
            "Person 1: I'm going to be from the city stop restaurant. I need to be picked up at the scott polar museum. \n",
            "Person 2: I have a grey honda for you. The contact number is 078226756. Can I help with anything else? \n",
            "Person 1: No, that's all I need. Thank you! \n",
            "Person 1: You're welcome. Enjoy your time in Cambridge!Skunfounfortustill still still open then then then then. \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E48tZxguCCo_",
        "outputId": "5c554a13-39c2-4904-f959-4ea03d57a1d6"
      },
      "source": [
        "sample_sentence = ' Person 1: '\n",
        "sample_sentence += input()\n",
        "sample_sentence += ' Person 2: '\n",
        "\n",
        "\n",
        "generate_dialogue(ReformerLM=model, model_state=STARTING_STATE, start_sentence=sample_sentence, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, max_len=120, temperature=0.2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Is there a restaurant nearby?\n",
            "Person 1: Is there a restaurant nearby?\n",
            "Person 2: : There are 110 restaurants in Cambridge. Do you have a price range in mind? \n",
            "Person 1: I'm looking for something expensive. \n",
            "Person 2: There are no restaurants in the area. Would you like to try another type of food? \n",
            "Person 1: How about Chinese food? \n",
            "Person 2: There are 14 Chinese restaurants in town. Do you have a particular area you'd like to dine in? \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}